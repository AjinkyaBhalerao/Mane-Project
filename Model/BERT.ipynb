{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\wissa\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.35.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.16.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.19.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.2)\n",
      "Requirement already satisfied: click in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: torch in c:\\users\\wissa\\anaconda3\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\wissa\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.9.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.23.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.7.1)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.59.3)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\wissa\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.23.4)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.1.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2023.7.22)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n",
      "Collecting transformers==4.12.2\n",
      "  Using cached transformers-4.12.2-py3-none-any.whl (3.1 MB)\n",
      "Requirement already satisfied: filelock in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from transformers==4.12.2) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.17 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from transformers==4.12.2) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from transformers==4.12.2) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from transformers==4.12.2) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from transformers==4.12.2) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from transformers==4.12.2) (2022.7.9)\n",
      "Requirement already satisfied: requests in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from transformers==4.12.2) (2.31.0)\n",
      "Collecting sacremoses (from transformers==4.12.2)\n",
      "  Obtaining dependency information for sacremoses from https://files.pythonhosted.org/packages/0b/f0/89ee2bc9da434bd78464f288fdb346bc2932f2ee80a90b2a4bbbac262c74/sacremoses-0.1.1-py3-none-any.whl.metadata\n",
      "  Using cached sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting tokenizers<0.11,>=0.10.1 (from transformers==4.12.2)\n",
      "  Using cached tokenizers-0.10.3.tar.gz (212 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from transformers==4.12.2) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.0.17->transformers==4.12.2) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.0.17->transformers==4.12.2) (4.7.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.12.2) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from requests->transformers==4.12.2) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from requests->transformers==4.12.2) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from requests->transformers==4.12.2) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from requests->transformers==4.12.2) (2023.7.22)\n",
      "Requirement already satisfied: click in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from sacremoses->transformers==4.12.2) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from sacremoses->transformers==4.12.2) (1.2.0)\n",
      "Using cached sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "Building wheels for collected packages: tokenizers\n",
      "  Building wheel for tokenizers (pyproject.toml): started\n",
      "  Building wheel for tokenizers (pyproject.toml): finished with status 'error'\n",
      "Failed to build tokenizers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for tokenizers (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [51 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-311\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "      copying py_src\\tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "      copying py_src\\tokenizers\\models\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "      copying py_src\\tokenizers\\decoders\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "      copying py_src\\tokenizers\\normalizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "      copying py_src\\tokenizers\\pre_tokenizers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "      copying py_src\\tokenizers\\processors\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "      copying py_src\\tokenizers\\trainers\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\base_tokenizer.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\bert_wordpiece.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\byte_level_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\char_level_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\sentencepiece_bpe.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\sentencepiece_unigram.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      copying py_src\\tokenizers\\implementations\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\implementations\n",
      "      creating build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "      copying py_src\\tokenizers\\tools\\visualizer.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "      copying py_src\\tokenizers\\tools\\__init__.py -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "      copying py_src\\tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\n",
      "      copying py_src\\tokenizers\\models\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\models\n",
      "      copying py_src\\tokenizers\\decoders\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\decoders\n",
      "      copying py_src\\tokenizers\\normalizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\normalizers\n",
      "      copying py_src\\tokenizers\\pre_tokenizers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\pre_tokenizers\n",
      "      copying py_src\\tokenizers\\processors\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\processors\n",
      "      copying py_src\\tokenizers\\trainers\\__init__.pyi -> build\\lib.win-amd64-cpython-311\\tokenizers\\trainers\n",
      "      copying py_src\\tokenizers\\tools\\visualizer-styles.css -> build\\lib.win-amd64-cpython-311\\tokenizers\\tools\n",
      "      running build_ext\n",
      "      running build_rust\n",
      "      error: can't find Rust compiler\n",
      "      \n",
      "      If you are using an outdated pip version, it is possible a prebuilt wheel is available for this package but pip is not able to install from it. Installing from the wheel would avoid the need for a Rust compiler.\n",
      "      \n",
      "      To update pip, run:\n",
      "      \n",
      "          pip install --upgrade pip\n",
      "      \n",
      "      and then retry package installation.\n",
      "      \n",
      "      If you did intend to build this package from source, try installing a Rust compiler from your system package manager and ensure it is on the PATH during installation. Alternatively, rustup (available at https://rustup.rs) is the recommended way to download and update the Rust compiler toolchain.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for tokenizers\n",
      "ERROR: Could not build wheels for tokenizers, which is required to install pyproject.toml-based projects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\wissa\\anaconda3\\lib\\site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from spacy) (8.2.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from spacy) (1.10.8)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers\n",
    "!pip install torch\n",
    "!pip install tensorflow\n",
    "!pip install transformers==4.12.2\n",
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import spacy\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most relevant answer: When Mane will share confidential information with another person or entity.  Also, when someone will observe or learn about Mane's confidential information. Additionally, when any of the parties (MANE and Counterparty) wish to disclose confidential information to each other.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "with open('modified_faq_data.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "\n",
    "user_question = input(\"Ask a question: \")\n",
    "\n",
    "# Tokenize and encode the user's question\n",
    "user_question_tokens = tokenizer.encode(user_question, truncation=True, max_length=512, return_tensors='pt')\n",
    "user_question_embedding = model(input_ids=user_question_tokens).last_hidden_state.mean(dim=1)\n",
    "\n",
    "# Calculate cosine similarity between the user's question and all questions in the JSON file\n",
    "similarities = []\n",
    "for item in data:\n",
    "    question_tokens = tokenizer.encode(item['question'], truncation=True, max_length=512, return_tensors='pt')\n",
    "    question_embedding = model(input_ids=question_tokens).last_hidden_state.mean(dim=1)\n",
    "\n",
    "    cosine_sim = torch.nn.functional.cosine_similarity(user_question_embedding, question_embedding)\n",
    "    similarities.append(cosine_sim.item())\n",
    "\n",
    "max_index = similarities.index(max(similarities))\n",
    "most_similar_answer = data[max_index]['answer']\n",
    "print(\"Most relevant answer:\", most_similar_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: What is the difference between a Commercial Representative and a Distributor?\n",
      "Cosine Similarity Score: 0.9281597137451172\n",
      "\n",
      "Question 2: What is the difference between a Commercial Representative and an Agent?\n",
      "Cosine Similarity Score: 0.9324036836624146\n",
      "\n",
      "Most relevant answer: An Agent is entitled to negotiate selling prices on behalf of the company it represents, while a Commercial Representative is not. MANE does not appoint Agents, only Commercial Representatives.\n",
      "Cosine Similarity Score with most relevant question: 0.9324036836624146\n",
      "The similarity scores are very close. Please be more specific in your question.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "with open('modified_faq_data.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "user_question = input(\"Ask a question: \")\n",
    "user_question_tokens = tokenizer.encode(user_question, truncation=True, max_length=512, return_tensors='pt')\n",
    "user_question_embedding = model(input_ids=user_question_tokens).last_hidden_state.mean(dim=1)\n",
    "user_question = input(\"Ask a question: \")\n",
    "threshold = 0.8\n",
    "similarities = []\n",
    "for i, item in enumerate(data):\n",
    "    question_tokens = tokenizer.encode(item['question'], truncation=True, max_length=512, return_tensors='pt')\n",
    "    question_embedding = model(input_ids=question_tokens).last_hidden_state.mean(dim=1)\n",
    "\n",
    "    cosine_sim = torch.nn.functional.cosine_similarity(user_question_embedding, question_embedding).item()\n",
    "\n",
    "    if cosine_sim > threshold:\n",
    "        print(f\"Question {i + 1}: {item['question']}\")\n",
    "        print(f\"Cosine Similarity Score: {cosine_sim}\")\n",
    "        print()\n",
    "\n",
    "        similarities.append(cosine_sim)\n",
    "\n",
    "if similarities:\n",
    "    sorted_similarities = sorted(similarities, reverse=True)\n",
    "    max_index = similarities.index(sorted_similarities[0])\n",
    "    most_similar_answer = data[max_index]['answer']\n",
    "\n",
    "    print(\"Most relevant answer:\", most_similar_answer)\n",
    "    print(\"Cosine Similarity Score with most relevant question:\", sorted_similarities[0])\n",
    "\n",
    "    # Check the difference between the top two similarity scores\n",
    "    score_difference = sorted_similarities[0] - sorted_similarities[1]\n",
    "\n",
    "    # Set a threshold for prompting the user for more specificity\n",
    "    specificity_threshold = 0.05\n",
    "\n",
    "    if score_difference < specificity_threshold:\n",
    "        print(\"The similarity scores are very close. Please be more specific in your question.\")\n",
    "else:\n",
    "    print(\"No relevant questions found above the threshold.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bert base model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 12: What is the purpose of the commercial contract review-process?\n",
      "Cosine Similarity Score: 0.789609432220459\n",
      "Question 78: What BI means in a CDA?\n",
      "Cosine Similarity Score: 0.780975341796875\n",
      "Cosine Similarity Score with refined question: 0.8781399130821228\n",
      "Most relevant answer: The contract review process provides for the conditions of verification and modifications of agreement proposals concerning the Mane Group. The purpose of this process is to make sure Mane actually wants and can comply with the terms and conditions offered/submitted to it, and that such terms and conditions are compliant with Mane's policies and operating methods. The Legal & IP Department proceeds with the final review of the concerned document based on the feedbacks of each concerned department at Mane.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "with open('modified_faq_data.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "user_question = input(\"Ask a question: \")\n",
    "\n",
    "user_question_tokens = tokenizer.encode(user_question, truncation=True, max_length=512, return_tensors='pt')\n",
    "user_question_embedding = model(input_ids=user_question_tokens).last_hidden_state.mean(dim=1)\n",
    "\n",
    "similarities = []\n",
    "for i, item in enumerate(data):\n",
    "    question_tokens = tokenizer.encode(item['question'], truncation=True, max_length=512, return_tensors='pt')\n",
    "    question_embedding = model(input_ids=question_tokens).last_hidden_state.mean(dim=1)\n",
    "\n",
    "    cosine_sim = torch.nn.functional.cosine_similarity(user_question_embedding, question_embedding).item()\n",
    "    similarities.append(cosine_sim)\n",
    "\n",
    "top_similar_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:2]\n",
    "\n",
    "for idx in top_similar_indices:\n",
    "    print(f\"Question {idx + 1}: {data[idx]['question']}\")\n",
    "    print(f\"Cosine Similarity Score: {similarities[idx]}\")\n",
    "\n",
    "user_response = input(\"Do you want to clarify your question for better results? (yes/no): \")\n",
    "\n",
    "if user_response.lower() == 'yes':\n",
    "    prev_question = data[top_similar_indices[0]]['question']\n",
    "    refined_question = input(\"Please refine your question: \")\n",
    "\n",
    "    refined_tokens = nlp(refined_question)\n",
    "    refined_lemmas = \" \".join([token.lemma_ for token in refined_tokens])\n",
    "\n",
    "    prev_tokens = nlp(prev_question)\n",
    "    prev_lemmas = \" \".join([token.lemma_ for token in prev_tokens])\n",
    "\n",
    "    lemmatized_question = f\"{refined_lemmas} {prev_lemmas}\"\n",
    "    lemmatized_tokens = tokenizer.encode(lemmatized_question, truncation=True, max_length=512, return_tensors='pt')\n",
    "    lemmatized_embedding = model(input_ids=lemmatized_tokens).last_hidden_state.mean(dim=1)\n",
    "\n",
    "    prev_tokens = tokenizer.encode(prev_question, truncation=True, max_length=512, return_tensors='pt')\n",
    "    prev_embedding = model(input_ids=prev_tokens).last_hidden_state.mean(dim=1)\n",
    "    lemmatized_cosine_sim = torch.nn.functional.cosine_similarity(lemmatized_embedding, prev_embedding).item()\n",
    "\n",
    "    print(f\"Cosine Similarity Score with refined question: {lemmatized_cosine_sim}\")\n",
    "    if lemmatized_cosine_sim > 0.8:\n",
    "        print(\"Most relevant answer:\", data[top_similar_indices[0]]['answer'])\n",
    "    else:\n",
    "        print(\"No relevant answer found for the refined question.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No relevant answer found for the original question.\n"
     ]
    }
   ],
   "source": [
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load spaCy for lemmatization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load your FAQ data\n",
    "with open('modified_faq_data.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Get user question\n",
    "user_question = input(\"Ask a question: \")\n",
    "\n",
    "# Tokenize and embed user question\n",
    "user_question_tokens = tokenizer.encode(user_question, truncation=True, max_length=512, return_tensors='pt')\n",
    "user_question_embedding = model(input_ids=user_question_tokens).last_hidden_state.mean(dim=1)\n",
    "\n",
    "# Calculate cosine similarity with each FAQ question\n",
    "similarities = []\n",
    "for i, item in enumerate(data):\n",
    "    question_tokens = tokenizer.encode(item['question'], truncation=True, max_length=512, return_tensors='pt')\n",
    "    question_embedding = model(input_ids=question_tokens).last_hidden_state.mean(dim=1)\n",
    "    cosine_sim = torch.nn.functional.cosine_similarity(user_question_embedding, question_embedding).item()\n",
    "    similarities.append(cosine_sim)\n",
    "\n",
    "# Sort and get top similar indices\n",
    "top_similar_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)\n",
    "\n",
    "# Function to handle responses based on cosine similarity\n",
    "def handle_response(threshold_low, threshold_high, top_indices):\n",
    "    for idx in top_indices:\n",
    "        print(f\"Question {idx + 1}: {data[idx]['question']}\")\n",
    "        print(f\"Answer: {data[idx]['answer']}\")\n",
    "        print(f\"Cosine Similarity Score: {similarities[idx]}\")\n",
    "        print()\n",
    "\n",
    "# Set thresholds and handle responses\n",
    "if similarities[top_similar_indices[0]] > 0.8:\n",
    "    # High similarity, return the first answer\n",
    "    handle_response(0.8, 1.0, top_similar_indices[:1])\n",
    "elif 0.5 < similarities[top_similar_indices[0]] <= 0.8:\n",
    "    # Moderate similarity, check the next 3 or 4 answers\n",
    "    handle_response(0.5, 0.8, top_similar_indices[:4])\n",
    "    # Check for very close scores and suggest refining the question\n",
    "    if similarities[top_similar_indices[0]] - similarities[top_similar_indices[1]] < 0.1:\n",
    "        user_response = input(\"Your question seems close to multiple FAQs. Do you want to refine your question for better results? (yes/no): \")\n",
    "        if user_response.lower() == 'yes':\n",
    "            prev_question = data[top_similar_indices[0]]['question']\n",
    "            refined_question = input(\"Please refine your question: \")\n",
    "\n",
    "            refined_tokens = nlp(refined_question)\n",
    "            refined_lemmas = \" \".join([token.lemma_ for token in refined_tokens])\n",
    "\n",
    "            prev_tokens = nlp(prev_question)\n",
    "            prev_lemmas = \" \".join([token.lemma_ for token in prev_tokens])\n",
    "\n",
    "            lemmatized_question = f\"{refined_lemmas} {prev_lemmas}\"\n",
    "            lemmatized_tokens = tokenizer.encode(lemmatized_question, truncation=True, max_length=512, return_tensors='pt')\n",
    "            lemmatized_embedding = model(input_ids=lemmatized_tokens).last_hidden_state.mean(dim=1)\n",
    "\n",
    "            prev_tokens = tokenizer.encode(prev_question, truncation=True, max_length=512, return_tensors='pt')\n",
    "            prev_embedding = model(input_ids=prev_tokens).last_hidden_state.mean(dim=1)\n",
    "            lemmatized_cosine_sim = torch.nn.functional.cosine_similarity(lemmatized_embedding, prev_embedding).item()\n",
    "\n",
    "            print(f\"Cosine Similarity Score with refined question: {lemmatized_cosine_sim}\")\n",
    "            \n",
    "            # Historical Scenario: If best distance score is less than 0.4\n",
    "            if lemmatized_cosine_sim < 0.4:\n",
    "                # Run LLM in inference GPT-3 (implementation required)\n",
    "                print(\"Running LLM in inference GPT-3...\")\n",
    "            else:\n",
    "                # Return the most relevant answer for the refined question\n",
    "                handle_response(0.8, 1.0, top_similar_indices[:1])\n",
    "        else:\n",
    "            print(\"No relevant answer found for the refined question.\")\n",
    "else:\n",
    "    print(\"No relevant answer found for the original question.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Bert large model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased')\n",
    "model = BertModel.from_pretrained('bert-large-uncased')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 36: When is a CDA needed?\n",
      "Cosine Similarity Score: 0.9674476385116577\n",
      "Question 50: Is a CDA needed for a facility visit?\n",
      "Cosine Similarity Score: 0.8730107545852661\n"
     ]
    }
   ],
   "source": [
    "with open('modified_faq_data.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "user_question = input(\"Ask a question: \")\n",
    "\n",
    "user_question_tokens = tokenizer.encode(user_question, truncation=True, max_length=512, return_tensors='pt')\n",
    "user_question_embedding = model(input_ids=user_question_tokens).last_hidden_state.mean(dim=1)\n",
    "\n",
    "similarities = []\n",
    "for i, item in enumerate(data):\n",
    "    question_tokens = tokenizer.encode(item['question'], truncation=True, max_length=512, return_tensors='pt')\n",
    "    question_embedding = model(input_ids=question_tokens).last_hidden_state.mean(dim=1)\n",
    "    cosine_sim = torch.nn.functional.cosine_similarity(user_question_embedding, question_embedding, dim=1).item()\n",
    "    similarities.append(cosine_sim)\n",
    "top_similar_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:2]\n",
    "for idx in top_similar_indices:\n",
    "    print(f\"Question {idx + 1}: {data[idx]['question']}\")\n",
    "    print(f\"Cosine Similarity Score: {similarities[idx]}\")\n",
    "\n",
    "user_response = input(\"Do you want to clarify your question for better results? (yes/no): \")\n",
    "\n",
    "if user_response.lower() == 'yes':\n",
    "    prev_question = data[top_similar_indices[0]]['question']\n",
    "    refined_question = input(\"Please refine your question: \")\n",
    "    refined_tokens = nlp(refined_question)\n",
    "    refined_lemmas = \" \".join([token.lemma_ for token in refined_tokens])\n",
    "\n",
    "    prev_tokens = nlp(prev_question)\n",
    "    prev_lemmas = \" \".join([token.lemma_ for token in prev_tokens])\n",
    "\n",
    "    lemmatized_question = f\"{refined_lemmas} {prev_lemmas}\"\n",
    "    lemmatized_tokens = tokenizer.encode(lemmatized_question, truncation=True, max_length=512, return_tensors='pt')\n",
    "    lemmatized_embedding = model(input_ids=lemmatized_tokens).last_hidden_state.mean(dim=1)\n",
    "\n",
    "    prev_tokens = tokenizer.encode(prev_question, truncation=True, max_length=512, return_tensors='pt')\n",
    "    prev_embedding = model(input_ids=prev_tokens).last_hidden_state.mean(dim=1)\n",
    "    lemmatized_cosine_sim = torch.nn.functional.cosine_similarity(lemmatized_embedding, prev_embedding, dim=1).item()\n",
    "\n",
    "    print(f\"Cosine Similarity Score with refined question: {lemmatized_cosine_sim}\")\n",
    "    if lemmatized_cosine_sim > 0.8:\n",
    "        print(\"Most relevant answer:\", data[top_similar_indices[0]]['answer'])\n",
    "    else:\n",
    "        print(\"No relevant answer found for the refined question.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 69: Where can I find the contract review form?\n",
      "Cosine Similarity Score: 0.5336938500404358\n",
      "Question 19: What are the steps to follow?\n",
      "Cosine Similarity Score: 0.5323439240455627\n",
      "Most relevant answer: The contract review form can be found by clicking on this link. 3 versions are available:  French, English and Spanish.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wissa\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:386: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 Answer: how are you ?\n",
      "\n",
      "A: I don't know. I think it's just a matter of time before we get to know each other. We've been friends for a long time, and we've had a lot of fun together. It's nice to be able to talk about things that are really important to you, but I'm not sure if we're going to get along as well as we would like to. But I do think that it would be nice for us to have\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel, GPT2LMHeadModel, GPT2Tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "lm_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "lm_model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def cosine_similarity_score(embedding1, embedding2):\n",
    "    return torch.nn.functional.cosine_similarity(embedding1, embedding2).item()\n",
    "\n",
    "def get_top_similar_indices(data, user_question_embedding, num_results=2):\n",
    "    similarities = [cosine_similarity_score(user_question_embedding, model(input_ids=bert_tokenizer.encode(data[idx]['question'], truncation=True, max_length=512, return_tensors='pt')).last_hidden_state.mean(dim=1)) for idx in range(len(data))]\n",
    "\n",
    "    top_similar_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:num_results]\n",
    "    return top_similar_indices, similarities\n",
    "\n",
    "def print_top_similar_questions(data, top_similar_indices, similarities):\n",
    "    for idx in top_similar_indices:\n",
    "        print(f\"Question {idx + 1}: {data[idx]['question']}\")\n",
    "        print(f\"Cosine Similarity Score: {similarities[idx]}\")\n",
    "\n",
    "def handle_response_threshold(similarity_scores, data, top_similar_indices):\n",
    "    if similarity_scores[0] < 0.5:\n",
    "        print(\"Most relevant answer:\", data[top_similar_indices[0]]['answer'])\n",
    "    elif 0.5 <= similarity_scores[0] < 0.8:\n",
    "        next_answers_indices = top_similar_indices[1:5]  \n",
    "        print_top_similar_questions(data, next_answers_indices, similarities)\n",
    "    else:\n",
    "        print(\"Scores are very close. Suggest refining the question.\")\n",
    "\n",
    "def handle_lack_of_context(data, top_similar_indices):\n",
    "    user_response = input(\"Do you want to clarify your question for better results? (yes/no): \")\n",
    "    if user_response.lower() == 'yes':\n",
    "        prev_question = data[top_similar_indices[0]]['question']\n",
    "        refined_question = input(\"Please refine your question: \")\n",
    "\n",
    "        refined_tokens = nlp(refined_question)\n",
    "        refined_lemmas = \" \".join([token.lemma_ for token in refined_tokens])\n",
    "\n",
    "        prev_tokens = nlp(prev_question)\n",
    "        prev_lemmas = \" \".join([token.lemma_ for token in prev_tokens])\n",
    "\n",
    "        lemmatized_question = f\"{refined_lemmas} {prev_lemmas}\"\n",
    "        lemmatized_tokens = bert_tokenizer.encode(lemmatized_question, truncation=True, max_length=512, return_tensors='pt')\n",
    "        lemmatized_embedding = bert_model(input_ids=lemmatized_tokens).last_hidden_state.mean(dim=1)\n",
    "\n",
    "        prev_tokens = bert_tokenizer.encode(prev_question, truncation=True, max_length=512, return_tensors='pt')\n",
    "        prev_embedding = bert_model(input_ids=prev_tokens).last_hidden_state.mean(dim=1)\n",
    "        lemmatized_cosine_sim = cosine_similarity_score(lemmatized_embedding, prev_embedding)\n",
    "\n",
    "        print(f\"Cosine Similarity Score with refined question: {lemmatized_cosine_sim}\")\n",
    "        if lemmatized_cosine_sim > 0.8:\n",
    "            print(\"Most relevant answer:\", data[top_similar_indices[0]]['answer'])\n",
    "        else:\n",
    "            print(\"No relevant answer found for the refined question.\")\n",
    "\n",
    "def handle_gpt2_response(data, top_similar_indices, similarities):\n",
    "    best_distance_score = similarities[0]\n",
    "    if best_distance_score < 0.5:\n",
    "        user_question = input(\"Ask a follow-up question for historical scenario: \")\n",
    "        # Run GPT-2 for inference with the user's question\n",
    "        lm_input = lm_tokenizer.encode(user_question, return_tensors='pt')\n",
    "        lm_output = lm_model.generate(lm_input, max_length=100, pad_token_id=lm_tokenizer.eos_token_id, num_beams=5, no_repeat_ngram_size=2, top_k=50, top_p=0.95)\n",
    "        lm_answer = lm_tokenizer.decode(lm_output[0], skip_special_tokens=True)\n",
    "        print(\"GPT-2 Answer:\", lm_answer)\n",
    "    else:\n",
    "        print(\"No need for GPT-2 in this scenario.\")\n",
    "\n",
    "with open('modified_faq_data.json', 'r') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "user_question = input(\"Ask a question: \")\n",
    "user_question_tokens = bert_tokenizer.encode(user_question, truncation=True, max_length=512, return_tensors='pt')\n",
    "user_question_embedding = bert_model(input_ids=user_question_tokens).last_hidden_state.mean(dim=1)\n",
    "\n",
    "top_similar_indices, similarities = get_top_similar_indices(data, user_question_embedding, num_results=2)\n",
    "print_top_similar_questions(data, top_similar_indices, similarities)\n",
    "handle_response_threshold(similarities, data, top_similar_indices)\n",
    "handle_lack_of_context(data, top_similar_indices)\n",
    "historical_scenario(data, top_similar_indices)\n",
    "handle_gpt2_response(data, top_similar_indices, similarities)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
