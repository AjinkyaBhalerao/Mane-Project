{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "import textwrap\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM\n",
    "import os\n",
    "import sys\n",
    "from typing import List\n",
    "\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    prepare_model_for_int8_training,\n",
    ")\n",
    "\n",
    "import fire\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(rc={'figure.figsize':(10, 7)})\n",
    "sns.set(rc={'figure.dpi':100})\n",
    "sns.set(style='white', palette='muted', font_scale=1.2)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://test.pypi.org/simple/\n",
      "Requirement already satisfied: bitsandbytes in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (0.39.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -i https://test.pypi.org/simple/ bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:14<00:00,  2.22it/s]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 222kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 142/142 [00:00<00:00, 771kB/s]\n",
      "Downloading tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 88.1MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 2.00/2.00 [00:00<00:00, 7.70kB/s]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "BASE_MODEL = \"baffo32/decapoda-research-llama-7B-hf\"\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL)\n",
    "\n",
    "tokenizer.pad_token_id = (\n",
    "    0  # unk. we want this to be different from the eos token\n",
    ")\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to file:///home/mane/.cache/huggingface/datasets/json/default-c7a1e248f1491b90/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 3194.44it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 635.69it/s]\n",
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to file:///home/mane/.cache/huggingface/datasets/json/default-c7a1e248f1491b90/0.0.0/0f7e3662623656454fcd2b650f34e886a7db4b9104504885bd462096cc7a9f51. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Loading a dataset cached in a LocalFileSystem is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/mane/Data/Mane-Project/Model/alpaca.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B37.26.187.6/home/mane/Data/Mane-Project/Model/alpaca.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m data \u001b[39m=\u001b[39m load_dataset(\u001b[39m\"\u001b[39;49m\u001b[39mjson\u001b[39;49m\u001b[39m\"\u001b[39;49m, data_files\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mmodified_faq_data.json\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B37.26.187.6/home/mane/Data/Mane-Project/Model/alpaca.ipynb#W3sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m data\n",
      "File \u001b[0;32m~/anaconda3/envs/chatbot/lib/python3.10/site-packages/datasets/load.py:1794\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, **config_kwargs)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1791\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[1;32m   1792\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[1;32m   1793\u001b[0m )\n\u001b[0;32m-> 1794\u001b[0m ds \u001b[39m=\u001b[39m builder_instance\u001b[39m.\u001b[39;49mas_dataset(split\u001b[39m=\u001b[39;49msplit, verification_mode\u001b[39m=\u001b[39;49mverification_mode, in_memory\u001b[39m=\u001b[39;49mkeep_in_memory)\n\u001b[1;32m   1795\u001b[0m \u001b[39m# Rename and cast features to match task schema\u001b[39;00m\n\u001b[1;32m   1796\u001b[0m \u001b[39mif\u001b[39;00m task \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/chatbot/lib/python3.10/site-packages/datasets/builder.py:1089\u001b[0m, in \u001b[0;36mDatasetBuilder.as_dataset\u001b[0;34m(self, split, run_post_process, verification_mode, ignore_verifications, in_memory)\u001b[0m\n\u001b[1;32m   1087\u001b[0m is_local \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m is_remote_filesystem(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fs)\n\u001b[1;32m   1088\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_local:\n\u001b[0;32m-> 1089\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoading a dataset cached in a \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fs)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1090\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_dir):\n\u001b[1;32m   1091\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\n\u001b[1;32m   1092\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDataset \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m: could not find data in \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_dir\u001b[39m}\u001b[39;00m\u001b[39m. Please make sure to call \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1093\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mbuilder.download_and_prepare(), or use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1094\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdatasets.load_dataset() before trying to access the Dataset object.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1095\u001b[0m     )\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Loading a dataset cached in a LocalFileSystem is not supported."
     ]
    }
   ],
   "source": [
    "data = load_dataset(\"json\", data_files=\"modified_faq_data.json\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
