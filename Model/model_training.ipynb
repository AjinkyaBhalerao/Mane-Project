{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FiIrqdto5zah",
        "outputId": "a429475d-7dea-4f96-acfd-a93c88004546"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import json\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load the DialoGPT model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\n",
        "\n",
        "# Load the training data\n",
        "with open(\"modified_faq_data.json\", \"r\") as f:\n",
        "    training_data = json.load(f)\n"
      ],
      "metadata": {
        "id": "wvUwe9g55rLz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the training data to a format that DialoGPT can understand\n",
        "training_inputs = []\n",
        "for item in training_data:\n",
        "  training_inputs.append(item['question'])\n",
        "  training_inputs.append(item['answer'])\n",
        "\n",
        "training_inputs[0:4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujcGtkF9FrRY",
        "outputId": "301c36e2-93e5-4214-a639-5e5cb1e74c96"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['What is the difference between a Commercial Representative and a Distributor?',\n",
              " 'A MANE Commercial Representative is appointed to assist for the promotion and sale of MANE’s products (fragrances/ flavours/ ingredients), within a limited territory (country, region, city…) for a specific market (fragrance/ flavour for the food/ cosmetic/ personal care/ home care, and less frequently, in fine fragrance, pharmaceutical, tobacco, pet food, and oral care industries), and grow MANE’s direct sales accordingly. A MANE Distributor is appointed to resell “as is” some of MANE’s products (fragrances/ flavours/ ingredients), within a limited territory (country, region, city…) for a specific market (fragrance/ flavour for the food/ cosmetic/ personal care/ home care, and less frequently, in fine fragrance, pharmaceutical, tobacco, pet food, and oral care industries) to its own customers.',\n",
              " 'What is the difference between a Commercial Representative and an Agent?',\n",
              " 'An Agent is entitled to negotiate selling prices on behalf of the company it represents, while a Commercial Representative is not. MANE does not appoint Agents, only Commercial Representatives.']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the training data\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "encoded_inputs = tokenizer(training_inputs, truncation=True, padding=True, return_tensors=\"pt\")\n",
        "\n",
        "# Train the DialoGPT model\n",
        "model.train()\n",
        "batch_size = 4\n",
        "\n",
        "for epoch in range(3):\n",
        "    for i in range(0, len(training_inputs), batch_size):\n",
        "        input_ids = encoded_inputs[\"input_ids\"][i:i + batch_size]\n",
        "        attention_mask = encoded_inputs[\"attention_mask\"][i:i + batch_size]\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "\n"
      ],
      "metadata": {
        "id": "KWDnPBNW6veE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "97y_n9FsI-Nq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "_-g5ZgZL2Mg-"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}