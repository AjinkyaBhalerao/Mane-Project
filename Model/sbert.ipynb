{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\wissa\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.35.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.16.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.1.99)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.19.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.3.2)\n",
      "Requirement already satisfied: click in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\wissa\\anaconda3\\lib\\site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**all-mpnet-base-v2** : All-round model tuned for many use-cases. Trained on a large and diverse dataset of over 1 billion training pairs.\n",
    "cosine-similarity (util.cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question is similar to: What is the difference between a Commercial Representative and a Distributor?\n",
      "User's input after clarification: what is the difference between a commercial and others ? what is the difference between a commercial and an agent ?\n",
      "Most similar question after clarification: What is the difference between a Commercial Representative and an Agent?\n",
      "Corresponding answer: An Agent is entitled to negotiate selling prices on behalf of the company it represents, while a Commercial Representative is not. MANE does not appoint Agents, only Commercial Representatives.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "import subprocess\n",
    "import shlex  # Import shlex to split the command string into a list\n",
    "# Load the lemmatizer for word normalization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Load the FAQ data\n",
    "with open('faq_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract questions and answers from the data\n",
    "# Extract questions and answers from the data\n",
    "filename = [entry['filename'] for entry in data]\n",
    "context = [entry['context'] for entry in data]\n",
    "questions = [entry.get('question', '') for entry in data]\n",
    "answers = [entry.get('answer', '') for entry in data]\n",
    "\n",
    "\n",
    "# Encode questions and answers\n",
    "filename_embeddings = model.encode(filename)\n",
    "context_embeddings = model.encode(context)\n",
    "question_embeddings = model.encode(questions)\n",
    "answer_embeddings = model.encode(answers)\n",
    "\n",
    "# Set the working directory to the FastChat folder\n",
    "fastchat_directory = '/home/mane/FastChat'\n",
    "vicuna_model_path = 'lmsys/vicuna-7b-v1.5'\n",
    "user_input = input(\"Type your question: \")\n",
    "\n",
    "# Encode the user's input\n",
    "user_input_embedding = model.encode(user_input)\n",
    "\n",
    "# Calculate cosine similarity between the user's input and all questions in the dataset\n",
    "similarities = util.cos_sim(user_input_embedding, question_embeddings)[0]\n",
    "most_similar_index = similarities.argmax()\n",
    "highest_similarity = similarities[most_similar_index]\n",
    "\n",
    "# Set the threshold values\n",
    "high_threshold = 0.8\n",
    "medium_threshold = 0.5\n",
    "\n",
    "# Construct the command to run Vicuna with the user's question\n",
    "\n",
    "# Check the similarity against different thresholds\n",
    "if highest_similarity >= high_threshold:\n",
    "    # High similarity, directly provide the answer\n",
    "    print(f\"User's input: {user_input}\")\n",
    "    print(f\"Most similar question: {questions[most_similar_index]}\")\n",
    "    print(f\"Corresponding answer: {answers[most_similar_index]}\")\n",
    "\n",
    "elif medium_threshold <= highest_similarity < high_threshold:\n",
    "    # Medium similarity, ask for clarification\n",
    "    print(f\"The question is similar to: {questions[most_similar_index]}\")\n",
    "    user_confirmation = input(\"Is this the same question? (yes/no): \").lower()\n",
    "\n",
    "    if user_confirmation == 'yes':\n",
    "        # User confirms, provide the answer\n",
    "        print(f\"User's input: {user_input}\")\n",
    "        print(f\"Most similar question: {questions[most_similar_index]}\")\n",
    "        print(f\"Corresponding answer: {answers[most_similar_index]}\")\n",
    "    else:\n",
    "        # User denies, ask for clarification\n",
    "        user_input += \" \" + input(\"Please provide more details or clarify your question: \")\n",
    "        user_input_embedding = model.encode(user_input)\n",
    "        similarities = util.cos_sim(user_input_embedding, question_embeddings)[0]\n",
    "        most_similar_index = similarities.argmax()\n",
    "        print(f\"User's input after clarification: {user_input}\")\n",
    "        print(f\"Most similar question after clarification: {questions[most_similar_index]}\")\n",
    "        print(f\"Corresponding answer: {answers[most_similar_index]}\")\n",
    "else:\n",
    "    # Low similarity, use Vicuna for answering\n",
    "    print(\"Low similarity. Asking Vicuna for an answer...\")\n",
    "\n",
    "    # Set the working directory to the FastChat folder\n",
    "    os.chdir('/home/mane/FastChat')\n",
    "\n",
    "    # Construct the command to run Vicuna with the user's question\n",
    "    vicuna_command = f'python3 -m fastchat.serve.cli --model {vicuna_model_path} \"{user_input}\"'\n",
    "    try:\n",
    "        # Use subprocess to run Vicuna and capture its output\n",
    "        vicuna_answer = subprocess.check_output(vicuna_command, cwd=fastchat_directory, shell=True)\n",
    "        print(f\"Vicuna's answer: {vicuna_answer.decode('utf-8')}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error while running Vicuna: {e}\")\n",
    "        vicuna_answer = \"Error while running Vicuna\"\n",
    "\n",
    "    print(f\"Vicuna's answer: {vicuna_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6143)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "highest_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vicuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "import subprocess\n",
    "import shlex  \n",
    "from test_vicuna import *\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Load the FAQ data\n",
    "with open('/home/mane/Mane-Project/Model/modified_faq_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract questions and answers from the data\n",
    "questions = [entry['question'] for entry in data]\n",
    "answers = [entry['answer'] for entry in data]\n",
    "\n",
    "# Encode questions and answers\n",
    "question_embeddings = model.encode(questions)\n",
    "answer_embeddings = model.encode(answers)\n",
    "\n",
    "# Set the working directory to the FastChat folder\n",
    "fastchat_directory = '/home/mane/FastChat'\n",
    "vicuna_model_path = 'lmsys/vicuna-7b-v1.5'\n",
    "user_input = input(\"Type your question: \")\n",
    "\n",
    "# Encode the user's input\n",
    "user_input_embedding = model.encode(user_input)\n",
    "\n",
    "# Calculate cosine similarity between the user's input and all questions in the dataset\n",
    "similarities = util.cos_sim(user_input_embedding, question_embeddings)[0]\n",
    "most_similar_index = similarities.argmax()\n",
    "highest_similarity = similarities[most_similar_index]\n",
    "\n",
    "# Set the threshold values\n",
    "high_threshold = 0.8\n",
    "medium_threshold = 0.5\n",
    "\n",
    "#appeler vicuna\n",
    "def appeler_vicuna(user_input):\n",
    "    model_path='/home/mane/model_weights/vicuna-7b-v1.5'\n",
    "    revision='main'\n",
    "    device='cuda'\n",
    "    gpus=None\n",
    "    num_gpus=1\n",
    "    max_gpu_memory=None\n",
    "    dtype=None\n",
    "    load_8bit=False\n",
    "    cpu_offloading=False\n",
    "    gptq_ckpt=None\n",
    "    gptq_wbits=16\n",
    "    gptq_groupsize=-1\n",
    "    gptq_act_order=False\n",
    "    awq_ckpt=None\n",
    "    awq_wbits=16\n",
    "    awq_groupsize=-1\n",
    "    enable_exllama=False\n",
    "    exllama_max_seq_len=4096\n",
    "    exllama_gpu_split=None\n",
    "    enable_xft=False\n",
    "    xft_max_seq_len=4096\n",
    "    xft_dtype=None\n",
    "    conv_template=None\n",
    "    conv_system_msg=None\n",
    "    temperature=0.7\n",
    "    repetition_penalty=1.0\n",
    "    max_new_tokens=2000\n",
    "    no_history=False\n",
    "    style='simple'\n",
    "    multiline=False\n",
    "    mouse=False\n",
    "    judge_sent_end=False\n",
    "    debug=False\n",
    "\n",
    "    gptq_config=GptqConfig(\n",
    "        ckpt=gptq_ckpt or model_path,\n",
    "        wbits=gptq_wbits,\n",
    "        groupsize=gptq_groupsize,\n",
    "        act_order=gptq_act_order,\n",
    "    )\n",
    "\n",
    "    awq_config=AWQConfig(\n",
    "        ckpt=awq_ckpt or model_path,\n",
    "        wbits=awq_wbits,\n",
    "        groupsize=awq_groupsize,\n",
    "    )\n",
    "\n",
    "    exllama_config=None,\n",
    "    xft_config=None,\n",
    "\n",
    "    chatio = SimpleChatIO(multiline)\n",
    "\n",
    "    model, tokenizer = load_model(\n",
    "        model_path,\n",
    "        device=device,\n",
    "        num_gpus=num_gpus,\n",
    "        max_gpu_memory=max_gpu_memory,\n",
    "        dtype=dtype,\n",
    "        load_8bit=load_8bit,\n",
    "        cpu_offloading=cpu_offloading,\n",
    "        gptq_config=gptq_config,\n",
    "        awq_config=awq_config,\n",
    "        exllama_config=False,\n",
    "        xft_config=False,\n",
    "        revision=revision,\n",
    "        debug=debug,\n",
    "    )\n",
    "\n",
    "    generate_stream_func = get_generate_stream_function(model, model_path)\n",
    "    conv = get_conversation_template(model_path)\n",
    "\n",
    "    # conv.roles[0] = 'USER'\n",
    "    # conv.roles[1] = 'ASSISTANT'\n",
    "\n",
    "    # user_message = \"Peux-tu me donner la liste des opérations pour changer une garniture mécanique sur une machine tournante ? \"\n",
    "\n",
    "    conv.append_message(conv.roles[0], user_input)\n",
    "    conv.append_message(conv.roles[1], None)\n",
    "\n",
    "    prompt = conv.get_prompt()\n",
    "\n",
    "    gen_params = {\n",
    "        \"model\": model_path,\n",
    "        \"prompt\": prompt,\n",
    "        \"temperature\": temperature,\n",
    "        \"repetition_penalty\": repetition_penalty,\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"stop\": conv.stop_str,\n",
    "        \"stop_token_ids\": conv.stop_token_ids,\n",
    "        \"echo\": False,\n",
    "    }\n",
    "\n",
    "    context_len = get_context_length(model.config)\n",
    "\n",
    "    output_stream = generate_stream_func(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        gen_params,\n",
    "        device,\n",
    "        context_len=context_len,\n",
    "        judge_sent_end=judge_sent_end,\n",
    "    )\n",
    "\n",
    "    # outputs = chatio.stream_output(output_stream)\n",
    "\n",
    "    outputs2 = chatio.stream_output2(output_stream)\n",
    "    return outputs2\n",
    "    \n",
    "\n",
    "# Construct the command to run Vicuna with the user's question\n",
    "\n",
    "# Check the similarity against different thresholds\n",
    "if highest_similarity >= high_threshold:\n",
    "    # High similarity, directly provide the answer\n",
    "    print(f\"User's input: {user_input}\")\n",
    "    print(f\"Most similar question: {questions[most_similar_index]}\")\n",
    "    print(f\"Corresponding answer: {answers[most_similar_index]}\")\n",
    "\n",
    "elif medium_threshold <= highest_similarity < high_threshold:\n",
    "    # Medium similarity, ask for clarification\n",
    "    print(f\"The question is similar to: {questions[most_similar_index]}\")\n",
    "    user_confirmation = input(\"Is this the same question? (yes/no): \").lower()\n",
    "\n",
    "    if user_confirmation == 'yes':\n",
    "        # User confirms, provide the answer\n",
    "        print(f\"User's input: {user_input}\")\n",
    "        print(f\"Most similar question: {questions[most_similar_index]}\")\n",
    "        print(f\"Corresponding answer: {answers[most_similar_index]}\")\n",
    "    else:\n",
    "        # User denies, ask for clarification\n",
    "        user_input += \" \" + input(\"Please provide more details or clarify your question: \")\n",
    "        user_input_embedding = model.encode(user_input)\n",
    "        similarities = util.cos_sim(user_input_embedding, question_embeddings)[0]\n",
    "        most_similar_index = similarities.argmax()\n",
    "        print(f\"User's input after clarification: {user_input}\")\n",
    "        print(f\"Most similar question after clarification: {questions[most_similar_index]}\")\n",
    "        print(f\"Corresponding answer: {answers[most_similar_index]}\")\n",
    "else:\n",
    "    print('Assistant:'+appeler_vicuna(user_input))\n",
    "    # # Low similarity, use Vicuna for answering\n",
    "    # print(\"Low similarity. Asking Vicuna for an answer...\")\n",
    "\n",
    "    # # Set the working directory to the FastChat folder\n",
    "    # os.chdir('/home/mane/FastChat')\n",
    "\n",
    "    # # Construct the command to run Vicuna with the user's question\n",
    "    # vicuna_command = f'python3 -m fastchat.serve.cli --model {vicuna_model_path} \"{user_input}\"'\n",
    "    # try:\n",
    "    #     # Use subprocess to run Vicuna and capture its output\n",
    "    #     vicuna_answer = subprocess.check_output(vicuna_command, cwd=fastchat_directory, shell=True)\n",
    "    #     print(f\"Vicuna's answer: {vicuna_answer.decode('utf-8')}\")\n",
    "    # except subprocess.CalledProcessError as e:\n",
    "    #     print(f\"Error while running Vicuna: {e}\")\n",
    "    #     vicuna_answer = \"Error while running Vicuna\"\n",
    "\n",
    "    # print(f\"Vicuna's answer: {vicuna_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "import subprocess\n",
    "import shlex\n",
    "import spacy\n",
    "\n",
    "# Load the lemmatizer for word normalization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "model = SentenceTransformer('multi-qa-mpnet-base-dot-v1', device='cuda')\n",
    "\n",
    "# Load the FAQ data\n",
    "with open('faq_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract questions and answers from the data\n",
    "filename = [entry['filename'] for entry in data]\n",
    "context = [entry['context'] for entry in data]\n",
    "questions = [entry.get('question', '') for entry in data]\n",
    "answers = [entry.get('answer', '') for entry in data]\n",
    "\n",
    "# Encode questions and answers\n",
    "filename_embeddings = model.encode(filename)\n",
    "context_embeddings = model.encode(context)\n",
    "question_embeddings = model.encode(questions)\n",
    "answer_embeddings = model.encode(answers)\n",
    "\n",
    "# Set the working directory to the FastChat folder\n",
    "fastchat_directory = '/home/mane/FastChat'\n",
    "vicuna_model_path = 'lmsys/vicuna-7b-v1.5'\n",
    "\n",
    "user_input = input(\"Type your question: \")\n",
    "while not user_input:\n",
    "    print(\"Please enter a valid question.\")\n",
    "    user_input = input(\"Type your question: \").strip()\n",
    "    \n",
    "# Initialize spaCy for lemmatization\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define a function for lemmatization\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_text = ' '.join([token.lemma_ for token in doc])\n",
    "    return lemmatized_text\n",
    "\n",
    "def provide_document():\n",
    "    return f\"The document context for this question is: {filename[most_similar_index]}\"\n",
    "chat_history = []\n",
    "\n",
    "def print_chat_history():\n",
    "    print(\"\\nChat History:\")\n",
    "    for entry in chat_history:\n",
    "        print(f\"{entry['user']}: {entry['message']}\")\n",
    "        \n",
    "# Encode the user's input\n",
    "user_input_embedding = model.encode(preprocess_text(user_input))\n",
    "\n",
    "# Calculate cosine similarity between the user's input and all questions in the dataset\n",
    "similarities = util.cos_sim(user_input_embedding, question_embeddings)[0]\n",
    "most_similar_index = similarities.argmax()\n",
    "highest_similarity = similarities[most_similar_index]\n",
    "\n",
    "# Set the threshold values\n",
    "high_threshold = 0.8\n",
    "medium_threshold = 0.5\n",
    "\n",
    "# Construct the command to run Vicuna with the user's question\n",
    "# Check the similarity against different thresholds\n",
    "if highest_similarity >= high_threshold:\n",
    "    # High similarity, directly provide the answer\n",
    "    chat_history.append((user_input, answers[most_similar_index]))\n",
    "    print(f\"User's input: {user_input}\")\n",
    "    print(f\"Most similar question: {questions[most_similar_index]}\")\n",
    "    print(f\"Corresponding answer: {answers[most_similar_index]}\")\n",
    "    print(f\"Similarity Score: {highest_similarity}\")\n",
    "    chat_history.append({'user': 'Chatbot', 'message': answers[most_similar_index]})\n",
    "\n",
    "    # Ask for user satisfaction\n",
    "    user_satisfaction = input(\"Is this the information you were looking for? (yes/no): \").lower()\n",
    "    if user_satisfaction == 'no':\n",
    "        # Continue the interaction loop\n",
    "        user_input += \" \" + input(\"Please provide more details or clarify your question: \")\n",
    "        user_input_embedding = model.encode(preprocess_text(user_input))\n",
    "        similarities = util.cos_sim(user_input_embedding, question_embeddings)[0]\n",
    "        most_similar_index = similarities.argmax()\n",
    "        print(f\"User's input after clarification: {user_input}\")\n",
    "        print(f\"Most similar question after clarification: {questions[most_similar_index]}\")\n",
    "        print(f\"Corresponding answer: {answers[most_similar_index]}\")\n",
    "        print(f\"Similarity Score after clarification: {similarities[most_similar_index]}\")\n",
    "        chat_history.append({'user': 'User', 'message': user_input})\n",
    "\n",
    "else:\n",
    "    if medium_threshold <= highest_similarity < high_threshold:\n",
    "        # Medium similarity, ask for clarification\n",
    "        print(f\"The question is similar to: {questions[most_similar_index]}\")\n",
    "        print(f\"Similarity Score: {highest_similarity}\")\n",
    "        user_confirmation = input(\"Is this the same question? (yes/no): \").lower()\n",
    "\n",
    "        if user_confirmation == 'yes':\n",
    "            # User confirms, provide the answer\n",
    "            print(f\"User's input: {user_input}\")\n",
    "            print(f\"Most similar question: {questions[most_similar_index]}\")\n",
    "            print(f\"Corresponding answer: {answers[most_similar_index]}\")\n",
    "            print(f\"Similarity Score: {highest_similarity}\")\n",
    "\n",
    "            # Ask for user satisfaction\n",
    "            user_satisfaction = input(\"Is this the information you were looking for? (yes/no): \").lower()\n",
    "            if user_satisfaction == 'no':\n",
    "                # Continue the interaction loop\n",
    "                user_input += \" \" + input(\"Please provide more details or clarify your question: \")\n",
    "                user_input_embedding = model.encode(preprocess_text(user_input))\n",
    "                similarities = util.cos_sim(user_input_embedding, question_embeddings)[0]\n",
    "                most_similar_index = similarities.argmax()\n",
    "                print(f\"User's input after clarification: {user_input}\")\n",
    "                print(f\"Most similar question after clarification: {questions[most_similar_index]}\")\n",
    "                print(f\"Corresponding answer: {answers[most_similar_index]}\")\n",
    "                print(f\"Similarity Score after clarification: {similarities[most_similar_index]}\")\n",
    "        else:\n",
    "            # User denies, ask for clarification\n",
    "            user_input += \" \" + input(\"Please provide more details or clarify your question: \")\n",
    "            user_input_embedding = model.encode(preprocess_text(user_input))\n",
    "            similarities = util.cos_sim(user_input_embedding, question_embeddings)[0]\n",
    "            most_similar_index = similarities.argmax()\n",
    "            print(f\"User's input after clarification: {user_input}\")\n",
    "            print(f\"Most similar question after clarification: {questions[most_similar_index]}\")\n",
    "            print(f\"Corresponding answer: {answers[most_similar_index]}\")\n",
    "            print(f\"Similarity Score after clarification: {similarities[most_similar_index]}\")\n",
    "    else:\n",
    "        # Low similarity, use Vicuna for answering\n",
    "        print(\"Low similarity. Asking Vicuna for an answer...\")\n",
    "\n",
    "        # Set the working directory to the FastChat folder\n",
    "        os.chdir('/home/mane/FastChat')\n",
    "\n",
    "        # Construct the command to run Vicuna with the user's question\n",
    "        vicuna_command = f'python3 -m fastchat.serve.cli --model {vicuna_model_path} \"{user_input}\"'\n",
    "        try:\n",
    "            # Use subprocess to run Vicuna and capture its output\n",
    "            vicuna_answer = subprocess.check_output(vicuna_command, cwd=fastchat_directory, shell=True)\n",
    "            print(f\"Vicuna's answer: {vicuna_answer.decode('utf-8')}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"Error while running Vicuna: {e}\")\n",
    "            vicuna_answer = \"Error while running Vicuna\"\n",
    "\n",
    "        print(f\"Vicuna's answer: {vicuna_answer}\")\n",
    "    print_chat_history()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Huggingface inference api - test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/BetterHF/vicuna-7b\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": \"how are you ?\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/reeducator/vicuna-13b-free\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
    "\treturn response.json()\n",
    "\t\n",
    "output = query({\n",
    "\t\"inputs\": \"how are you ? \",\n",
    "})\n",
    "\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
