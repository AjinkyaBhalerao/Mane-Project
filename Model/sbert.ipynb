{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from sentence-transformers) (4.34.1)\n",
      "Requirement already satisfied: tqdm in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: torchvision in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from sentence-transformers) (0.16.1)\n",
      "Requirement already satisfied: numpy in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from sentence-transformers) (1.26.2)\n",
      "Requirement already satisfied: scikit-learn in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: scipy in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from sentence-transformers) (1.11.3)\n",
      "Requirement already satisfied: nltk in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from sentence-transformers) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from sentence-transformers) (0.1.97)\n",
      "Requirement already satisfied: huggingface-hub>=0.4.0 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from sentence-transformers) (0.17.3)\n",
      "Requirement already satisfied: filelock in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2023.10.0)\n",
      "Requirement already satisfied: requests in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.8.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: sympy in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (2.18.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (12.1.105)\n",
      "Requirement already satisfied: triton==2.1.0 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.6.0->sentence-transformers) (12.3.52)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.10.3)\n",
      "Requirement already satisfied: tokenizers<0.15,>=0.14 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.14.1)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.4.0)\n",
      "Requirement already satisfied: click in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from nltk->sentence-transformers) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from torchvision->sentence-transformers) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/mane/anaconda3/envs/chatbot/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**all-mpnet-base-v2** : All-round model tuned for many use-cases. Trained on a large and diverse dataset of over 1 billion training pairs.\n",
    "cosine-similarity (util.cos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vicuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low similarity. Asking Vicuna for an answer...\n",
      "user: ASSISTANT: Hello! As an AI language model, I don't have emotions or experiences, but I'm here to assist you with any questions or tasks you may have. How can I help you today?\n",
      "chatbot: USER: exit...\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import json\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "import subprocess\n",
    "import shlex\n",
    "import spacy\n",
    "\n",
    "# Load the lemmatizer for word normalization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "model = SentenceTransformer('multi-qa-mpnet-base-dot-v1', device='cuda')\n",
    "\n",
    "# Load the FAQ data\n",
    "with open('/home/mane/Mane-Project/Model/faq_data.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extract questions and answers from the data\n",
    "filename = [entry['filename'] for entry in data]\n",
    "context = [entry['context'] for entry in data]\n",
    "questions = [entry.get('question', '') for entry in data]\n",
    "answers = [entry.get('answer', '') for entry in data]\n",
    "\n",
    "# Encode questions and answers\n",
    "filename_embeddings = model.encode(filename)\n",
    "context_embeddings = model.encode(context)\n",
    "question_embeddings = model.encode(questions)\n",
    "answer_embeddings = model.encode(answers)\n",
    "\n",
    "# Set the working directory to the FastChat folder\n",
    "fastchat_directory = '/home/mane/FastChat'\n",
    "vicuna_model_path = 'lmsys/vicuna-7b-v1.5'\n",
    "\n",
    "user_input = input(\"Type your question: \")\n",
    "while not user_input:\n",
    "    print(\"Please enter a valid question.\")\n",
    "    user_input = input(\"Type your question: \").strip()\n",
    "    \n",
    "# Initialize spaCy for lemmatization\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define a function for lemmatization\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmatized_text = ' '.join([token.lemma_ for token in doc])\n",
    "    return lemmatized_text\n",
    "\n",
    "def provide_document():\n",
    "    return f\"The document context for this question is: {filename[most_similar_index]}\"\n",
    "chat_history = []\n",
    "\n",
    "def print_chat_history():\n",
    "    print(\"\\nChat History:\")\n",
    "    for entry in chat_history:\n",
    "        print(f\"{entry['user']}: {entry['message']}\")\n",
    "        \n",
    "# Encode the user's input\n",
    "user_input_embedding = model.encode(preprocess_text(user_input))\n",
    "\n",
    "# Calculate cosine similarity between the user's input and all questions in the dataset\n",
    "similarities = util.cos_sim(user_input_embedding, question_embeddings)[0]\n",
    "most_similar_index = similarities.argmax()\n",
    "highest_similarity = similarities[most_similar_index]\n",
    "\n",
    "# Set the threshold values\n",
    "high_threshold = 0.8\n",
    "medium_threshold = 0.5\n",
    "\n",
    "# Construct the command to run Vicuna with the user's question\n",
    "# Check the similarity against different thresholds\n",
    "if highest_similarity >= high_threshold:\n",
    "    # High similarity, directly provide the answer\n",
    "    print(f\"User's input: {user_input}\")\n",
    "    print(f\"Most similar question: {questions[most_similar_index]}\")\n",
    "    print(f\"Corresponding answer: {answers[most_similar_index]}\")\n",
    "    print(f\"Similarity Score: {highest_similarity}\")\n",
    "\n",
    "    # Ask for user satisfaction\n",
    "    user_satisfaction = input(\"Is this the information you were looking for? (yes/no): \").lower()\n",
    "    if user_satisfaction == 'no':\n",
    "        # Continue the interaction loop\n",
    "        user_input += \" \" + input(\"Please provide more details or clarify your question: \")\n",
    "        user_input_embedding = model.encode(preprocess_text(user_input))\n",
    "        similarities = util.cos_sim(user_input_embedding, question_embeddings)[0]\n",
    "        most_similar_index = similarities.argmax()\n",
    "        print(f\"User's input after clarification: {user_input}\")\n",
    "        print(f\"Most similar question after clarification: {questions[most_similar_index]}\")\n",
    "        print(f\"Corresponding answer: {answers[most_similar_index]}\")\n",
    "        print(f\"Similarity Score after clarification: {similarities[most_similar_index]}\")\n",
    "\n",
    "else:\n",
    "    if medium_threshold <= highest_similarity < high_threshold:\n",
    "        # Medium similarity, ask for clarification\n",
    "        print(f\"The question is similar to: {questions[most_similar_index]}\")\n",
    "        print(f\"Similarity Score: {highest_similarity}\")\n",
    "        user_confirmation = input(\"Is this the same question? (yes/no): \").lower()\n",
    "\n",
    "        if user_confirmation == 'yes':\n",
    "            # User confirms, provide the answer\n",
    "            print(f\"User's input: {user_input}\")\n",
    "            print(f\"Most similar question: {questions[most_similar_index]}\")\n",
    "            print(f\"Corresponding answer: {answers[most_similar_index]}\")\n",
    "            print(f\"Similarity Score: {highest_similarity}\")\n",
    "\n",
    "            # Ask for user satisfaction\n",
    "            user_satisfaction = input(\"Is this the information you were looking for? (yes/no): \").lower()\n",
    "            if user_satisfaction == 'no':\n",
    "                # Continue the interaction loop\n",
    "                user_input += \" \" + input(\"Please provide more details or clarify your question: \")\n",
    "                user_input_embedding = model.encode(preprocess_text(user_input))\n",
    "                similarities = util.cos_sim(user_input_embedding, question_embeddings)[0]\n",
    "                most_similar_index = similarities.argmax()\n",
    "                print(f\"User's input after clarification: {user_input}\")\n",
    "                print(f\"Most similar question after clarification: {questions[most_similar_index]}\")\n",
    "                print(f\"Corresponding answer: {answers[most_similar_index]}\")\n",
    "                print(f\"Similarity Score after clarification: {similarities[most_similar_index]}\")\n",
    "        else:\n",
    "            # User denies, ask for clarification\n",
    "            user_input += \" \" + input(\"Please provide more details or clarify your question: \")\n",
    "            user_input_embedding = model.encode(preprocess_text(user_input))\n",
    "            similarities = util.cos_sim(user_input_embedding, question_embeddings)[0]\n",
    "            most_similar_index = similarities.argmax()\n",
    "            print(f\"User's input after clarification: {user_input}\")\n",
    "            print(f\"Most similar question after clarification: {questions[most_similar_index]}\")\n",
    "            print(f\"Corresponding answer: {answers[most_similar_index]}\")\n",
    "            print(f\"Similarity Score after clarification: {similarities[most_similar_index]}\")\n",
    "    else:\n",
    "        # Low similarity, use Vicuna for answering\n",
    "        print(\"Low similarity. Asking Vicuna for an answer...\")\n",
    "\n",
    "        # Set the working directory to the FastChat folder\n",
    "        os.chdir('/home/mane/FastChat')\n",
    "\n",
    "        # Construct the command to run Vicuna with the user's question\n",
    "        vicuna_command = ['python3', '-m', 'fastchat.serve.cli', '--model-path', f'lmsys/vicuna-7b-v1.5']\n",
    "        try:\n",
    "            # Use subprocess to run Vicuna and capture its output\n",
    "            vicuna_process = subprocess.Popen(vicuna_command, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "            vicuna_out, vicuna_err = vicuna_process.communicate(input=user_input)\n",
    "            \n",
    "            if vicuna_process.returncode == 0:\n",
    "                # Extract user's question and Vicuna's answer from the output\n",
    "                lines = vicuna_out.strip().split('\\n')\n",
    "                user_question = lines[0].split('USER: ')[-1].strip()\n",
    "                vicuna_answer = lines[-1].split('ASSISTANT: ')[-1].strip()\n",
    "                \n",
    "                # Display the extracted information in the desired format\n",
    "                print(f\"user: {user_question}\")\n",
    "                print(f\"chatbot: {vicuna_answer}\")\n",
    "            else:\n",
    "                print(f\"Error while running Vicuna: {vicuna_err}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error while running Vicuna: {e}\")\n",
    "            vicuna_answer = \"Error while running Vicuna\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
